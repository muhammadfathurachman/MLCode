q()
library(party)
library(lattice)
library(nutshell)
data(births2006.smpl)
d <- births2006.smpl
str(d)
t1 <- table(d$DMETH_REC, d$WTGAIN)
t1
t1 <- table(d$DMETH_REC, d$DOB_MM)
t1
barplot(t1)
barplot(t1, horiz = TRUE)
T1 <- t1[,-2]
T1
T1 <- t1[-2,]
T1
barplot(t1, horiz = TRUE)
barplot(t1, horiz = TRUE)
barplot(T1, horiz = TRUE)
barplot(T1, horiz = FALSE)
str(d)
t2 <- table(d$APGAR5, d$SEX)
t2
str(d)
t3 <- table(d$WTGAIN, d$DOB_WK)
t3
t3 <- table(d$APGAR5, d$DOB_WK)
t3
t3 <- table(d$SEX, d$DOB_WK)
t3
t3 <- table(d$SEX, d$DMETH_REC)
t3
str(d)
hist(d$WTGAIN)
hist(d$DBWT)
hist(d$DBWT)
source("http://www.bioconductor.org/biocLite.R")
> biocLite("GEOquery")
biocLite("GEOquery")
library(RWeka)
library(DAAG)
library(lattice)
library(DAAG)
library(rpart.plot)
library(rpart)
library(rattle)
setwd("G:/COMPUTER SCIENCE LECTURE/MACHINE LEARNING UI/TUGAS 1")
car <- read.csv("car_e.csv",sep = ";", header = TRUE)
library(mlbench)
install.packages("mlbench")
library(mlbench)
for(i in 1:10)
{
#PartitionData using K-Folds Validation
testIndex <- which(folds == i, arr.ind = TRUE)
testData <- car[testIndex,]
trainData <- car[-testIndex,]
model <- NaiveBayes(class~., data = trainData)
prediction <- predict(fit, testData[,1:6], type = "class")
table(prediction, testData$class)
}
folds <- cut(seq(1,nrow(car)),breaks=10,labels=FALSE)
for(i in 1:10)
{
#PartitionData using K-Folds Validation
testIndex <- which(folds == i, arr.ind = TRUE)
testData <- car[testIndex,]
trainData <- car[-testIndex,]
model <- NaiveBayes(class~., data = trainData)
prediction <- predict(fit, testData[,1:6], type = "class")
table(prediction, testData$class)
}
for(i in 1:10)
{
#PartitionData using K-Folds Validation
testIndex <- which(folds == i, arr.ind = TRUE)
testData <- car[testIndex,]
trainData <- car[-testIndex,]
model <- naiveBayes(class~., data = trainData)
prediction <- predict(fit, testData[,1:6], type = "class")
table(prediction, testData$class)
}
library(mlbench)
for(i in 1:10)
{
#PartitionData using K-Folds Validation
testIndex <- which(folds == i, arr.ind = TRUE)
testData <- car[testIndex,]
trainData <- car[-testIndex,]
model <- naiveBayes(class~., data = trainData)
prediction <- predict(fit, testData[,1:6], type = "class")
table(prediction, testData$class)
}
library(e1071)
for(i in 1:10)
{
#PartitionData using K-Folds Validation
testIndex <- which(folds == i, arr.ind = TRUE)
testData <- car[testIndex,]
trainData <- car[-testIndex,]
model <- naiveBayes(class~., data = trainData)
prediction <- predict(fit, testData[,1:6], type = "class")
table(prediction, testData$class)
}
for(i in 1:10)
{
#PartitionData using K-Folds Validation
testIndex <- which(folds == i, arr.ind = TRUE)
testData <- car[testIndex,]
trainData <- car[-testIndex,]
model <- naiveBayes(class~., data = trainData)
prediction <- predict(model, testData[,1:6], type = "class")
table(prediction, testData$class)
}
}
prediction
table(prediction, testData$class)
table(prediction, testData$class)
sum(prediction)
table(prediction, testData$class)
model <- naiveBayes(class~., data = trainData)
prediction <- predict(model, testData[,1:6], type = "class")
table(prediction, testData$class)
testIndex <- which(folds == i, arr.ind = TRUE)
testData <- car[testIndex,]
trainData <- car[-testIndex,]
model <- naiveBayes(class~., data = trainData)
prediction <- predict(model, testData[,1:6], type = "class")
table(prediction, testData$class)
for(i in 1:10)
{
#PartitionData using K-Folds Validation
testIndex <- which(folds == i, arr.ind = TRUE)
testData <- car[testIndex,]
trainData <- car[-testIndex,]
model <- naiveBayes(class~., data = trainData)
prediction <- predict(model, testData[,1:6], type = "class")
table(prediction, testData$class)
}
netTab[i]<- table(prediction, testData$class)
netTab <- c()
for(i in 1:10)
{
#PartitionData using K-Folds Validation
testIndex <- which(folds == i, arr.ind = TRUE)
testData <- car[testIndex,]
trainData <- car[-testIndex,]
model <- naiveBayes(class~., data = trainData)
prediction <- predict(model, testData[,1:6], type = "class")
netTab[i]<- table(prediction, testData$class)
}
prediction <- predict(model, testData[,1:6], type = "class")
netTab[i]<- table(prediction, testData$class)
prediction[1,1]
prediction <- predict(fit, testData[,1:6], type = "class")
for(i in 1:10)
{
#PartitionData using K-Folds Validation
testIndex <- which(folds == i, arr.ind = TRUE)
testData <- car[testIndex,]
trainData <- car[-testIndex,]
fit <- rpart(class~., data = trainData)
prediction <- predict(fit, testData[,1:6], type = "class")
table(prediction, testData$class)
}
View(prediction)
tablePred <- table(prediction, testData$class)
tablePred
tablePred[1,1]
(12+20+93+24)/173
1-((12+20+93+24)/173)
tableTree<- table(predictionTree, testData$class)
tableTree <-c()
accTree <- c()
tableTree<- table(predictionTree, testData$class)
tableBayes <-c()
for(i in 1:10)
{
#PartitionData using K-Folds Validation
testIndex <- which(folds == i, arr.ind = TRUE)
testData <- car[testIndex,]
trainData <- car[-testIndex,]
#Predition
fit <- rpart(class~., data = trainData)
predictionTee <- predict(fit, testData[,1:6], type = "class")
tableTree<- table(predictionTree, testData$class)
#NaiveBayes
modelBayes<- naiveBayes(class~., data = trainData)
predictionBayes <- predict(modelBayes, testData[,1:6], type="class")
tableBayes<- table(predictionBayes, testData$class)
}
for(i in 1:10)
{
#PartitionData using K-Folds Validation
testIndex <- which(folds == i, arr.ind = TRUE)
testData <- car[testIndex,]
trainData <- car[-testIndex,]
#Predition
fit <- rpart(class~., data = trainData)
predictionTree <- predict(fit, testData[,1:6], type = "class")
tableTree<- table(predictionTree, testData$class)
#NaiveBayes
modelBayes<- naiveBayes(class~., data = trainData)
predictionBayes <- predict(modelBayes, testData[,1:6], type="class")
tableBayes<- table(predictionBayes, testData$class)
}
accBayes <- c()
accBayes[i] <- tableBayes[1,1]+tableBayes[2,2]+tableBayes[3,3]+tableBayes[4,4]
accTree[i] <- tableTree[1,1]+tableTree[2,2]+tableTree[3,3]+tableTree[4,4]
for(i in 1:10)
{
#PartitionData using K-Folds Validation
testIndex <- which(folds == i, arr.ind = TRUE)
testData <- car[testIndex,]
trainData <- car[-testIndex,]
#Predition
fit <- rpart(class~., data = trainData)
predictionTree <- predict(fit, testData[,1:6], type = "class")
tableTree<- table(predictionTree, testData$class)
accTree[i] <- (tableTree[1,1]+tableTree[2,2]+tableTree[3,3]+tableTree[4,4])/nrow(testData)
#NaiveBayes
modelBayes<- naiveBayes(class~., data = trainData)
predictionBayes <- predict(modelBayes, testData[,1:6], type="class")
tableBayes<- table(predictionBayes, testData$class)
accBayes[i] <- (tableBayes[1,1]+tableBayes[2,2]+tableBayes[3,3]+tableBayes[4,4])/nrow(testData)
}
mean(accTree)
mean(accBayes)
tableErro <- matrix(data = NA, nrow = 10, ncol = 2)
tableError <- matrix(data = NA, nrow = 10, ncol = 2)
colMeans(tableError)
for(i in 1:10)
{
#PartitionData using K-Folds Validation
testIndex <- which(folds == i, arr.ind = TRUE)
testData <- car[testIndex,]
trainData <- car[-testIndex,]
#Predition
fit <- rpart(class~., data = trainData)
predictionTree <- predict(fit, testData[,1:6], type = "class")
tableTree<- table(predictionTree, testData$class)
accTree[i] <- (tableTree[1,1]+tableTree[2,2]+tableTree[3,3]+tableTree[4,4])/nrow(testData)
tableError[i,1] <- accTree[i]
#NaiveBayes
modelBayes<- naiveBayes(class~., data = trainData)
predictionBayes <- predict(modelBayes, testData[,1:6], type="class")
tableBayes<- table(predictionBayes, testData$class)
accBayes[i] <- (tableBayes[1,1]+tableBayes[2,2]+tableBayes[3,3]+tableBayes[4,4])/nrow(testData)
tableError[i,2] <- accBayes[i]
}
colMeans(tableError)
tableError
tableError <- matrix(data = NA, nrow = 10, ncol = 4)
for(i in 1:10)
{
#PartitionData using K-Folds Validation
testIndex <- which(folds == i, arr.ind = TRUE)
testData <- car[testIndex,]
trainData <- car[-testIndex,]
#Predition
fit <- rpart(class~., data = trainData)
predictionTree <- predict(fit, testData[,1:6], type = "class")
tableTree<- table(predictionTree, testData$class)
accTree[i] <- (tableTree[1,1]+tableTree[2,2]+tableTree[3,3]+tableTree[4,4])/nrow(testData)
tableError[i,1] <- accTree[i]
tableError[i,2] <- 1-accTree[i]
#NaiveBayes
modelBayes<- naiveBayes(class~., data = trainData)
predictionBayes <- predict(modelBayes, testData[,1:6], type="class")
tableBayes<- table(predictionBayes, testData$class)
accBayes[i] <- (tableBayes[1,1]+tableBayes[2,2]+tableBayes[3,3]+tableBayes[4,4])/nrow(testData)
tableError[i,3] <- accBayes[i]
tableError[i,4] <- 1-accBayes[i]
}
colMeans(tableError)
tableError
weather <- read.csv("weather.csv", header = TRUE, sep = ";")
View(weather)
modelTree <- rpart(PlayTennis~.,data = weather)
View(modelTree)
plot(modelTree)
str(modelTree)
summary(modelTree)
plot(fit)
plot(modelTree, uniform = TRUE)
library(lattice)
library(DAAG)
library(rpart.plot)
library(rpart)
library(mlbench)
library(rattle)
library(RColorBrewer)
setwd("G:/COMPUTER SCIENCE LECTURE/MACHINE LEARNING UI/TUGAS 1")
car <- read.csv("car_e.csv",sep = ";", header = TRUE)
folds <- cut(seq(1,nrow(car)),breaks=10,labels=FALSE)
accTree <- c()
accBayes <- c()
accBayes <c()
tableTree <-c()
prediction
tableBayes <-c()
tableError <- matrix(data = NA, nrow = 10, ncol = 4)
for(i in 1:10)
{
#PartitionData using K-Folds Validation
testIndex <- which(folds == i, arr.ind = TRUE)
testData <- car[testIndex,]
trainData <- car[-testIndex,]
#Predition
fit <- rpart(class~., data = trainData)
predictionTree <- predict(fit, testData[,1:6], type = "class")
tableTree<- table(predictionTree, testData$class)
accTree[i] <- (tableTree[1,1]+tableTree[2,2]+tableTree[3,3]+tableTree[4,4])/nrow(testData)
tableError[i,1] <- accTree[i]
tableError[i,2] <- 1-accTree[i]
#NaiveBayes
modelBayes<- naiveBayes(class~., data = trainData)
predictionBayes <- predict(modelBayes, testData[,1:6], type="class")
tableBayes<- table(predictionBayes, testData$class)
accBayes[i] <- (tableBayes[1,1]+tableBayes[2,2]+tableBayes[3,3]+tableBayes[4,4])/nrow(testData)
tableError[i,3] <- accBayes[i]
tableError[i,4] <- 1-accBayes[i]
}
colMeans(tableError)
weather <- read.csv("weather.csv", header = TRUE, sep = ";")
p <- rpart(PlayTennis~., data = weather)
plot(p)
p <- rpart(PlayTennis~., data = weather, control=rpart.control(minsplit=2, minbucket=1, cp=0.001))
plot(p)
p <- rpart(PlayTennis~., data = weather,method = "class", control=rpart.control(minsplit=2, minbucket=1, cp=0.001))
plot(p)
fancyRpartPlot(p)
p <- rpart(PlayTennis~., data = weather,method = "class", control=rpart.control(minsplit=2, minbucket=1, cp=0))
fancyRpartPlot(p)
fancyRpartPlot(p)
p <- rpart(PlayTennis~., data = weather,method = "class")
fancyRpartPlot(p)
p <- rpart(PlayTennis~., data = weather,method = "class", control=rpart.control(minsplit=2, cp=0))
fancyRpartPlot(p)
fit <- rpart(class~., data = trainData)
fancyRpartPlot(fit)
fit <- rpart(class~., data = trainData, method = "class",control=rpart.control(minsplit=4, minbucket=1, cp=0.001))
predictionTree <- predict(fit, testData[,1:6], type = "class")
tableTree<- table(predictionTree, testData$class)
accTree[i] <- (tableTree[1,1]+tableTree[2,2]+tableTree[3,3]+tableTree[4,4])/nrow(testData)
tableError[i,1] <- accTree[i]
tableError[i,2] <- 1-accTree[i]
accTree
mean(accTree)
fancyRpartPlot(fit)
fancyRpartPlot(p)
install.packages("RWekajars")
library(RWekajars)
